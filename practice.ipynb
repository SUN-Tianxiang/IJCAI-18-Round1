{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0de2b9a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "合并后的数据总行数: 496509\n",
      "开始提取时间特征...\n",
      "时序特征提取与数值化完成。\n"
     ]
    }
   ],
   "source": [
    "# 导入必要的库\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# 文件路径\n",
    "TRAIN_PATH = './data/round1_ijcai_18_train_20180301.txt'\n",
    "TEST_PATH = './data/round1_ijcai_18_test_a_20180301.txt'\n",
    "# 读取数据\n",
    "train_df = pd.read_csv(TRAIN_PATH, sep=r'\\s+')\n",
    "test_df = pd.read_csv(TEST_PATH, sep=r'\\s+')\n",
    "# 记录原始训练集长度，以便后续分割\n",
    "train_len = len(train_df)\n",
    "# 合并训练集和测试集\n",
    "data_df = pd.concat([train_df.drop('is_trade', axis=1), test_df], ignore_index=True)\n",
    "# 添加原始索引列，便于后续还原顺序\n",
    "data_df['original_index'] = data_df.index\n",
    "\n",
    "print(f'合并后的数据总行数: {len(data_df)}')\n",
    "\n",
    "print('开始提取时间特征...')\n",
    "# 1. 基础时间特征提取（使用Datetime对象）\n",
    "data_df['context_timestamp'] = pd.to_datetime(data_df['context_timestamp'], unit='s')\n",
    "data_df['context_hour'] = data_df['context_timestamp'].dt.hour\n",
    "data_df['context_weekday'] = data_df['context_timestamp'].dt.dayofweek\n",
    "data_df['context_month'] = data_df['context_timestamp'].dt.month\n",
    "# 2. 计算深度时序特征\n",
    "data_df = data_df.sort_values(by=['user_id', 'context_timestamp']).reset_index(drop=True)\n",
    "# 计算用户和店铺的时间间隔特征（Last Interaction Delta）\n",
    "for col in ['user_id', 'shop_id']:\n",
    "    # 获取前一条记录的时间戳\n",
    "    data_df[f'{col}_prev_time'] = data_df.groupby(col)['context_timestamp'].shift(1)\n",
    "    # 计算时间间隔（Timedelta 对象）\n",
    "    data_df[f'{col}_time_gap'] = data_df['context_timestamp'] - data_df[f'{col}_prev_time']\n",
    "    # 删除临时列\n",
    "    data_df.drop(f'{col}_prev_time', axis=1, inplace=True)\n",
    "# 3. 将时间特征数值化（转换为秒，float）\n",
    "# Timedelta 列转换为秒\n",
    "data_df['user_delta_time'] = data_df.groupby('user_id')['context_timestamp'].diff().dt.total_seconds().fillna(-1)\n",
    "data_df['user_id_time_gap'] = data_df['user_id_time_gap'].dt.total_seconds().fillna(-1)\n",
    "data_df['shop_id_time_gap'] = data_df['shop_id_time_gap'].dt.total_seconds().fillna(-1)\n",
    "# 将原始时间戳转换为数值（用于模型训练）\n",
    "data_df['context_timestamp'] = data_df['context_timestamp'].astype(np.int64) // 10**9\n",
    "# 还原数据顺序\n",
    "data_df = data_df.sort_values(by='original_index').reset_index(drop=True)\n",
    "data_df.drop('original_index', axis=1, inplace=True)\n",
    "\n",
    "print('时序特征提取与数值化完成。')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3efe4c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始解析 item_category_list...\n",
      "item_category_list 解析完成。\n"
     ]
    }
   ],
   "source": [
    "# --- 序列特征：item_category_list ---\n",
    "print('开始解析 item_category_list...')\n",
    "# 提取主类目（第一个ID），如果列表为空则返回 NaN 或一个占位符\n",
    "def get_first_category(cat_list):\n",
    "    try:\n",
    "        return cat_list.split(';')[0]\n",
    "    except:\n",
    "        return np.nan\n",
    "# 提取次级类目（第二个ID）\n",
    "def get_second_category(cat_list):\n",
    "    try:\n",
    "        # 如果列表中只有1个元素，split(';')[1]会出错，需try-except处理\n",
    "        parts = cat_list.split(';')\n",
    "        if len(parts) > 1:\n",
    "            return parts[1]\n",
    "        return np.nan\n",
    "    except:\n",
    "        return np.nan\n",
    "# 提取类目深度（ID数量）\n",
    "data_df['item_category_depth'] = data_df['item_category_list'].apply(\n",
    "    lambda x: len(x.split(';')) if pd.notna(x) else 0\n",
    ")\n",
    "# 应用函数提取主/次类目\n",
    "data_df['item_category_1'] = data_df['item_category_list'].apply(get_first_category)\n",
    "data_df['item_category_2'] = data_df['item_category_list'].apply(get_second_category)\n",
    "# 删除原始列\n",
    "data_df.drop('item_category_list', axis=1, inplace=True)\n",
    "\n",
    "print('item_category_list 解析完成。')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5364e44a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始解析 item_property_list...\n",
      "item_property_list 解析完成。\n"
     ]
    }
   ],
   "source": [
    "# --- 序列特征：item_property_list ---\n",
    "print('开始解析 item_property_list...')\n",
    "# 提取属性数量\n",
    "data_df['item_property_count'] = data_df['item_property_list'].apply(\n",
    "    lambda x: len(x.split(';')) if pd.notna(x) else 0\n",
    ")\n",
    "# 可选：提取第一个属性ID作为新特征\n",
    "def get_first_property(prop_list):\n",
    "    try:\n",
    "        return prop_list.split(';')[0]\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "data_df['item_property_1'] = data_df['item_property_list'].apply(get_first_property)\n",
    "# 删除原始列\n",
    "data_df.drop('item_property_list', axis=1, inplace=True)\n",
    "\n",
    "print('item_property_list 解析完成。')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c72f056c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始构建计数特征...\n",
      "特征 user_id_count 构建完成。\n",
      "特征 item_id_count 构建完成。\n",
      "特征 shop_id_count 构建完成。\n",
      "特征 item_category_1_count 构建完成。\n",
      "特征 item_brand_id_count 构建完成。\n",
      "特征 item_city_id_count 构建完成。\n",
      "计数特征构建完成。\n"
     ]
    }
   ],
   "source": [
    "# --- 计数特征 (Count Features) ---\n",
    "print(\"开始构建计数特征...\")\n",
    "# 需要统计的ID字段列表\n",
    "id_features = [\n",
    "    'user_id', 'item_id', 'shop_id', \n",
    "    'item_category_1', 'item_brand_id', 'item_city_id'\n",
    "]\n",
    "for col in id_features:\n",
    "    # 构建新列名，例如：user_id_count\n",
    "    new_col = f'{col}_count'\n",
    "    # 使用 value_counts() 统计频次，并 map 回原 DataFrame\n",
    "    count_map = data_df[col].value_counts().to_dict()\n",
    "    data_df[new_col] = data_df[col].map(count_map)\n",
    "    # 打印部分结果以供检查\n",
    "    print(f\"特征 {new_col} 构建完成。\")\n",
    "\n",
    "print(\"计数特征构建完成。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86461caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始构建交叉计数特征...\n",
      "特征 user_id__shop_id_count 构建完成。\n",
      "特征 user_id__item_category_1_count 构建完成。\n",
      "特征 user_id__item_brand_id_count 构建完成。\n",
      "特征 item_id__context_page_id_count 构建完成。\n",
      "交叉计数特征构建完成。\n"
     ]
    }
   ],
   "source": [
    "# --- 交叉计数特征 (Cross Count Features) ---\n",
    "print(\"开始构建交叉计数特征...\")\n",
    "# 需要统计交叉频次的特征对列表\n",
    "cross_features = [\n",
    "    ('user_id', 'shop_id'),\n",
    "    ('user_id', 'item_category_1'),\n",
    "    ('user_id', 'item_brand_id'),\n",
    "    ('item_id', 'context_page_id')\n",
    "]\n",
    "\n",
    "for col1, col2 in cross_features:\n",
    "    new_col = f'{col1}__{col2}_count'\n",
    "    # 使用 groupby 进行交叉计数\n",
    "    gp = data_df.groupby([col1, col2]).size()\n",
    "    gp = gp.reset_index().rename(columns={0: new_col})\n",
    "    # 合并回原 DataFrame\n",
    "    data_df = data_df.merge(gp, on=[col1, col2], how='left')\n",
    "\n",
    "    print(f\"特征 {new_col} 构建完成。\")\n",
    "\n",
    "print(\"交叉计数特征构建完成。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65c0643a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始构建转化率特征...\n",
      "特征 user_id_CTR 构建完成。\n",
      "特征 shop_id_CTR 构建完成。\n",
      "特征 item_category_1_CTR 构建完成。\n",
      "转化率特征构建完成。\n"
     ]
    }
   ],
   "source": [
    "# --- 拆分回训练集和测试集 ---\n",
    "train_processed = data_df.iloc[:train_len].copy()\n",
    "test_processed = data_df.iloc[train_len:].copy()\n",
    "# 将训练集的 is_trade 列合并回 train_processed\n",
    "train_processed['is_trade'] = train_df['is_trade']\n",
    "# --- 转化率特征 (Converrsion Rate Features) ---\n",
    "print('开始构建转化率特征...')\n",
    "# 需要计算转化率的特征列表\n",
    "cr_features = ['user_id', 'shop_id', 'item_category_1']\n",
    "\n",
    "for col in cr_features:\n",
    "    new_col = f'{col}_CTR' # CTR 表示 Click-Through Rate，这里是交易率\n",
    "    # 1. 在训练集上进行 Groupby 聚合\n",
    "    #  agg(['mean']) 计算 is_trade 的均值即为转化率\n",
    "    gp = train_processed.groupby(col)['is_trade'].agg(['mean']).reset_index()\n",
    "    gp.rename(columns={'mean': new_col}, inplace=True)\n",
    "    # 2. 将转化率特征合并回训练集和测试集\n",
    "    train_processed = pd.merge(train_processed, gp, on=col, how='left')\n",
    "    test_processed = pd.merge(test_processed, gp, on=col, how='left')\n",
    "\n",
    "    print(f\"特征 {new_col} 构建完成。\")\n",
    "\n",
    "print('转化率特征构建完成。')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6151d21e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始对高基数ID特征进行 Label Encoding...\n",
      "特征 item_category_1 编码完成。\n",
      "特征 item_category_2 编码完成。\n",
      "特征 item_property_1 编码完成。\n",
      "特征 item_brand_id 编码完成。\n",
      "特征 item_city_id 编码完成。\n",
      "所有类别特征编码完成。\n",
      "训练集特征维度: (478138, 42)\n",
      "测试集特征维度: (18371, 42)\n"
     ]
    }
   ],
   "source": [
    "# 1. 识别并删除不需要的列\n",
    "# ID特征通常不需要直接输入树模型，但它们可以作为类别特征\n",
    "# 先删除原始的、高基数的ID列，仅保留作为统计特征的ID列\n",
    "DROP_COLS = [\n",
    "    'instance_id', 'user_id', 'item_id', 'shop_id', 'context_id',\n",
    "    'item_category_list', 'item_property_list', 'predict_category_property'\n",
    "    # 之前已经删除了 item_category_list 和 item_property_list，这里确保其他高基数ID也处理\n",
    "]\n",
    "# 找到当前DataFrame中存在的ID列进行删除（防止重复删除报错）\n",
    "cols_to_drop = [col for col in DROP_COLS if col in train_processed.columns]\n",
    "x_train = train_processed.drop(cols_to_drop + ['is_trade'], axis=1)\n",
    "y_train = train_processed['is_trade']\n",
    "x_test = test_processed.drop(cols_to_drop, axis=1)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# --- 识别需要编码的特征类别 ---\n",
    "CATEGORY_FEATURES = [\n",
    "    'item_category_1', \n",
    "    'item_category_2', \n",
    "    'item_property_1',\n",
    "    # 还可以加入其他高基数的ID特征，如 item_brand_id, item_city_id, etc.\n",
    "    'item_brand_id', 'item_city_id' \n",
    "]\n",
    "\n",
    "print(\"开始对高基数ID特征进行 Label Encoding...\")\n",
    "# 循环对需要编码的特征进行处理\n",
    "for col in CATEGORY_FEATURES:\n",
    "    # 确保列存在，并将其类型转换为字符串（以便于LablelEncoder处理）\n",
    "    if col in x_train.columns:\n",
    "        x_train[col] = x_train[col].astype(str)\n",
    "        x_test[col] = x_test[col].astype(str)\n",
    "        # 将训练集和测试集的当前列合并，确保编码器能看到所有可能的ID值\n",
    "        full_data = pd.concat([x_train[col], x_test[col]], ignore_index=True)\n",
    "\n",
    "        le = LabelEncoder()\n",
    "        # fit on full data\n",
    "        le.fit(full_data)\n",
    "        # transform back\n",
    "        x_train[col] = le.transform(x_train[col])\n",
    "        x_test[col] = le.transform(x_test[col])\n",
    "        # 将数据类型强制转换为整数\n",
    "        x_train[col] = x_train[col].astype(int)\n",
    "        x_test[col] = x_test[col].astype(int)\n",
    "\n",
    "        print(f\"特征 {col} 编码完成。\")\n",
    "\n",
    "print(\"所有类别特征编码完成。\")\n",
    "# 2. 处理缺失值\n",
    "# 树模型对NaN值不敏感，但为了保险起见，填充一个特殊值\n",
    "# 对于类别特征，用一个特殊的负值或‘NaN’字符串填充；对于数值特征，用-999等特殊值填充\n",
    "x_train = x_train.fillna(-999)\n",
    "x_test = x_test.fillna(-999)\n",
    "\n",
    "x_test = x_test[x_train.columns]\n",
    "\n",
    "print(f\"训练集特征维度: {x_train.shape}\")\n",
    "print(f\"测试集特征维度: {x_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72732152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始交叉验证训练...\n",
      "Fold 1 - LogLoss: 0.028820, AUC: 0.993530\n",
      "Fold 2 - LogLoss: 0.024109, AUC: 0.994388\n",
      "Fold 3 - LogLoss: 0.024649, AUC: 0.993860\n",
      "Fold 4 - LogLoss: 0.025440, AUC: 0.993496\n",
      "Fold 5 - LogLoss: 0.024550, AUC: 0.993784\n",
      "Overall - LogLoss: 0.025514, AUC: 0.993803\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, log_loss\n",
    "# 定义模型评估指标：AUC 和 LogLoss\n",
    "# LightGBM 参数设置（基于经验）\n",
    "lgb_params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc', # 仍然使用 AUC 作为 early_stopping 的监控指标，更稳定\n",
    "    'learning_rate': 0.03, # ⚠️ 降低学习率（原 0.05）\n",
    "    'num_leaves': 31,\n",
    "    'max_depth': -1,\n",
    "    'seed': 42,\n",
    "    'n_estimators': 2000, # ⚠️ 增加迭代次数（原 1000）\n",
    "    'colsample_bytree': 0.8,\n",
    "    'subsample': 0.8,\n",
    "    'reg_alpha': 0.5,  # ⚠️ 增加 L1 正则化（原 0.1）\n",
    "    'reg_lambda': 0.5, # ⚠️ 增加 L2 正则化（原 0.1）\n",
    "    'n_jobs': -1,\n",
    "    'verbose': -1\n",
    "}\n",
    "# --- 交叉验证训练（K-Fold Cross-Validation）---\n",
    "NFOLDS = 5\n",
    "folds = StratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=42)\n",
    "\n",
    "oof_preds = np.zeros(x_train.shape[0]) # Out-of-Fold 预测，用于模型融合\n",
    "sub_preds = np.zeros(x_test.shape[0])  # 测试集预测\n",
    "\n",
    "print(\"开始交叉验证训练...\")\n",
    "\n",
    "for n_fold, (train_idx, valid_idx) in enumerate(folds.split(x_train, y_train)):\n",
    "    x_train_fold, y_train_fold = x_train.iloc[train_idx], y_train.iloc[train_idx]\n",
    "    x_valid_fold, y_valid_fold = x_train.iloc[valid_idx], y_train.iloc[valid_idx]\n",
    "    # 初始化并训练模型\n",
    "    model = lgb.LGBMClassifier(**lgb_params)\n",
    "    model.fit(x_train_fold, y_train_fold,\n",
    "              eval_set=[(x_valid_fold, y_valid_fold)],\n",
    "              callbacks=[lgb.early_stopping(stopping_rounds=50, verbose=False)])\n",
    "    # 在验证集上进行预测\n",
    "    oof_preds[valid_idx] = model.predict_proba(x_valid_fold)[:, 1]\n",
    "    sub_preds += model.predict_proba(x_test)[:, 1] / folds.n_splits\n",
    "    valid_preds = model.predict_proba(x_valid_fold)[:, 1]\n",
    "    oof_preds[valid_idx] = valid_preds\n",
    "    # 计算并打印当前折的评估指标\n",
    "    valid_logloss = log_loss(y_valid_fold, valid_preds)\n",
    "    valid_auc = roc_auc_score(y_valid_fold, valid_preds)\n",
    "\n",
    "    print(f'Fold {n_fold + 1} - LogLoss: {valid_logloss:.6f}, AUC: {valid_auc:.6f}')\n",
    "# --- 结果评估 --\n",
    "overall_logloss = log_loss(y_train, oof_preds) # 计算整体 LogLoss\n",
    "overall_auc = roc_auc_score(y_train, oof_preds)\n",
    "print(f'Overall - LogLoss: {overall_logloss:.6f}, AUC: {overall_auc:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d06aec81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "前 20 个重要特征:\n",
      "                           feature  importance\n",
      "40                     shop_id_CTR         472\n",
      "22                shop_id_time_gap         449\n",
      "21                user_id_time_gap         414\n",
      "39                     user_id_CTR         403\n",
      "10               context_timestamp         218\n",
      "29                   user_id_count         175\n",
      "35          user_id__shop_id_count         172\n",
      "16             shop_score_delivery         150\n",
      "18                    context_hour         148\n",
      "38  item_id__context_page_id_count         145\n",
      "0                    item_brand_id         143\n",
      "30                   item_id_count         140\n",
      "31                   shop_id_count         140\n",
      "17          shop_score_description         127\n",
      "27             item_property_count         122\n",
      "15              shop_score_service         109\n",
      "9                  user_star_level          95\n",
      "13       shop_review_positive_rate          94\n",
      "23                 user_delta_time          79\n",
      "37    user_id__item_brand_id_count          78\n"
     ]
    }
   ],
   "source": [
    "# 提取特征名和重要性得分\n",
    "feature_importances = pd.DataFrame(\n",
    "    {'feature': x_train.columns, 'importance': model.feature_importances_}\n",
    ").sort_values(by='importance', ascending=False)\n",
    "# 打印前 20 个重要特征\n",
    "print(\"前 20 个重要特征:\")\n",
    "print(feature_importances.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db48356b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "提交文件已保存至 ./submission.csv\n"
     ]
    }
   ],
   "source": [
    "# --- 生成提交文件 ---\n",
    "submission = pd.DataFrame({\n",
    "    'instance_id': test_df['instance_id'],\n",
    "    'is_trade': sub_preds\n",
    "})\n",
    "\n",
    "SUBMISSION_PATH = './submission.csv'\n",
    "submission.to_csv(SUBMISSION_PATH, index=False, float_format='%.16f') # 强制保留小数点后 10 位，确保精度\n",
    "print(f\"提交文件已保存至 {SUBMISSION_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ijcai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
